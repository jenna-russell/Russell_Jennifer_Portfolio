{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Perceptron</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.4\n"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "#ion()\n",
    "import numpy as np\n",
    "from matplotlib import *\n",
    "#matplotlib.use('PDF')\n",
    "from pylab import *\n",
    "#</GRADED>\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# add p02 folder\n",
    "sys.path.insert(0, './p02/')\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def perceptronUpdate(x,y,w):\n",
    "    \"\"\"\n",
    "    function w=perceptronUpdate(x,y,w);\n",
    "    \n",
    "    Implementation of Perceptron weights updating\n",
    "    Input:\n",
    "    x : input vector of d dimensions (d)\n",
    "    y : corresponding label (-1 or +1)\n",
    "    w : weight vector of d dimensions\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector after updating (d)\n",
    "    \"\"\"\n",
    "    assert(y in {-1,1})\n",
    "    assert(len(w.shape)==1), \"At the update w must be a vector not a matrix (try w=w.flatten())\"\n",
    "    assert(len(x.shape)==1), \"At the update x must be a vector not a matrix (try x=x.flatten())\"\n",
    "    \n",
    "    \n",
    "    if y == 1:\n",
    "        w = w + x\n",
    "    else:\n",
    "        w =w - x\n",
    "    \n",
    "    return w.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def perceptron(xs,ys):\n",
    "    \"\"\"\n",
    "    function w=perceptron(xs,ys);\n",
    "    \n",
    "    Implementation of a Perceptron classifier\n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd)\n",
    "    ys : n labels (-1 or +1)\n",
    "    \n",
    "    Output:\n",
    "    w : weight vector (1xd)\n",
    "    b : bias term\n",
    "    \"\"\"\n",
    "\n",
    "    assert(len(xs.shape)==2), \"The first input to Perceptron must be a _matrix_ of row input vecdtors.\"\n",
    "    assert(len(ys.shape)==1), \"The second input to Perceptron must be a _vector_ of n labels (try ys.flatten()).\"\n",
    "        \n",
    "    n, d = xs.shape     # so we have n input vectors, of d dimensions each\n",
    "    ys.flatten()\n",
    "\n",
    "    \n",
    "    #initialize w as vector of 0's, t = 0\n",
    "    #w = np.zeros(d+1)\n",
    "    w = np.zeros(d+1)\n",
    "    #ones = np.ones([n,1], dtype=int)\n",
    "    #xs = np.concatenate((xs, ones), axis=1)\n",
    "    a = np.ones((n,d+1))\n",
    "    a[:,:-1] = xs\n",
    "    xs = a\n",
    "    t=0\n",
    "\n",
    "    \n",
    "    #while there is i in [m] s.t. y_i(w dot x) less than or equal to 0\n",
    "    #somehow have to randomly iterate through all, when does it converges\n",
    "    condition_met = False\n",
    "    \n",
    "    while t<100 and condition_met == False: \n",
    "        #create new list of indices, shuffle that, and use that for x and y\n",
    "        indice_list = np.arange(n)\n",
    "        np.random.shuffle(indice_list)\n",
    "        condition_met = True\n",
    "        \n",
    "        for i in indice_list: \n",
    "            #print(ys[i]*(xs[i,:].dot(w.T)))\n",
    "            if (ys[i]*(xs[i,:].dot(w.T))) <= 0:\n",
    "                w = perceptronUpdate(xs[i],ys[i],w)\n",
    "                condition_met = False\n",
    "        t = t + 1\n",
    "                 \n",
    "    return (w[:-1],w[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def classifyLinear(xs,w,b):\n",
    "    \"\"\"\n",
    "    function preds=classifyLinear(xs,w,b)\n",
    "    \n",
    "    Make predictions with a linear classifier\n",
    "    Input:\n",
    "    xs : n input vectors of d dimensions (nxd) [could also be a single vector of d dimensions]\n",
    "    w : weight vector of dimensionality d\n",
    "    b : bias (scalar)\n",
    "    \n",
    "    Output:\n",
    "    preds: predictions (1xn)\n",
    "    \"\"\"    \n",
    "    w = w.flatten()    \n",
    "    predictions=np.zeros(xs.shape[0])\n",
    "\n",
    "    \n",
    "    predictions = xs @ w + b \n",
    "    predictions[predictions <= 0] = -1\n",
    "    predictions[predictions > 0] = 1\n",
    "    \n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 input emails.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 256)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the email and hashes the symbols into a vector\n",
    "def extractfeaturesnaive(path, B):\n",
    "    with open(path, 'r') as femail:\n",
    "        # initialize all-zeros feature vector\n",
    "        v = np.zeros(B)\n",
    "        email = femail.read()\n",
    "        # breaks for non-ascii characters\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            v[hash(token) % B] = 1\n",
    "    return v\n",
    "\n",
    "def loadspamdata(extractfeatures, B=256, path=\"../resource/lib/public/new_train_data/\"):\n",
    "    '''\n",
    "    INPUT:\n",
    "    extractfeatures : function to extract features\n",
    "    B               : dimensionality of feature space\n",
    "    path            : the path of folder to be processed\n",
    "    \n",
    "    OUTPUT:\n",
    "    X, Y\n",
    "    '''\n",
    "    if path[-1] != '/':\n",
    "        path += '/'\n",
    "    \n",
    "    with open(path + 'index', 'r') as f:\n",
    "        allemails = [x for x in f.read().split('\\n') if ' ' in x]\n",
    "    \n",
    "    xs = np.zeros((len(allemails), B))\n",
    "    ys = np.zeros(len(allemails))\n",
    "    for i, line in enumerate(allemails):\n",
    "        label, filename = line.split(' ')\n",
    "        # make labels +1 for \"spam\" and -1 for \"ham\"\n",
    "        ys[i] = (label == 'spam') * 2 - 1\n",
    "        xs[i, :] = extractfeatures(path + filename, B)\n",
    "    print('Loaded %d input emails.' % len(ys))\n",
    "    return xs, ys\n",
    "\n",
    "X,Y = loadspamdata(extractfeaturesnaive)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validation_split(X, Y, k):\n",
    "    # Split data into training and validation\n",
    "    n, d = X.shape\n",
    "    cutoff = int(np.ceil(0.8 * n))\n",
    "    # indices of training samples\n",
    "    xTr = X[:cutoff,:]\n",
    "    yTr = Y[:cutoff]\n",
    "    # indices of validation samples\n",
    "    xTv = X[cutoff:,:]\n",
    "    yTv = Y[cutoff:]\n",
    "\n",
    "    \n",
    "    return xTr, yTr, xTv, yTv\n",
    "\n",
    "\n",
    "xTr, yTr, xTv, yTv = validation_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feature_dimension = 256\n",
    "\n",
    "def extractfeaturescomp(path, B):\n",
    "    '''\n",
    "    INPUT:\n",
    "    path : file path of email\n",
    "    B    : dimensionality of feature vector\n",
    "    \n",
    "    OUTPUTS:\n",
    "    x    : B dimensional vector\n",
    "    '''\n",
    "    ## naive implementation (same as extractfeaturesnaive(path, B))\n",
    "    x = np.zeros(B)\n",
    "    with open(path, 'r') as femail:\n",
    "        email = femail.read()\n",
    "        tokens = email.split()\n",
    "        for token in tokens:\n",
    "            x[hash(token) % B] = 1\n",
    "            \n",
    "    \n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def trainspamfiltercomp(xTr, yTr):\n",
    "    '''\n",
    "    INPUT:\n",
    "    xTr : nxd dimensional matrix (each row is an input vector)\n",
    "    yTr : d   dimensional vector (each entry is a label)\n",
    "    \n",
    "    OUTPUTS:\n",
    "    w : d dimensional vector for linear classification\n",
    "    '''\n",
    "    w = np.random.rand(np.shape(xTr)[1])\n",
    "    b = np.random.rand()\n",
    "    \n",
    "\n",
    "    \n",
    "    w,b = perceptron(xTr,yTr)\n",
    "    \n",
    "    return w,b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Evaluate the performance on validation set\n",
    "wTr, bTr = trainspamfiltercomp(xTr, yTr)\n",
    "predictions = classifyLinear(xTr, wTr,bTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.931"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rate of accuracy\n",
    "np.sum(predictions == yTr)/len(yTr)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
